
The KPU Domain Flow Architecture is different from a Stored Program Machine. In Domain Flow Architecture machines, there is a data movement and a
compute element. The data movement data path creates a block-oriented system level schedule, that is, the SURE algorithm analysis will derive an optimal
schedule of data movement that the data path needs to create. The compute fabric will be programmed to 'react' to the data streams that arrive. The
compute fabric processing elements are data flow execution engines, with a small CAM that matches tokens, and a passive ALU that executes any instruction
token that shows up at its input port. The CAM will compose these instruction tokens when all operands have arrived and dispatch them to the ALUs. The
result tokens are routed to their destination CAM via a network overlay mechanism, the virtual distance being defined by the spatial projection used to
reduce the dimensionality of the SURE program to the dimensionality of the compute fabric. The most important functionality we need to focus on is not
the compute ISA, as that is simply the typical set of ALU operations required for linear algebra, signal processing, and constraint solving, but the data
movement ISA. The data movement ISA needs to be a productive set of actions that will configure the DMAs, BLockMovers, and Streamers to create the
system level schedule that defines our execution. This is way we first looked at the tiling decompositions for differently structured tensor operations,
and where we discovered that depending on the shape of these tensors we will find that one of the input or weight stationary schedule is best, or the
output stationary. This will determine the system level schedule that the data movement data path needs to create. Let's start with the output stationary
system schedules, because they are easier to fuse.

This is an excellent clarification of the Domain Flow Architecture paradigm. 

  Domain Flow Architecture vs Stored Program Machine

  | Aspect   | Stored Program Machine      | Domain Flow Architecture                     |
  |----------|-----------------------------|----------------------------------------------|
  | Control  | PC-driven instruction fetch | Data-driven reaction                         |
  | Compute  | Instructions fetch operands | Operands arrive as tokens, trigger execution |
  | Schedule | Runtime dynamic             | Compile-time SURE-derived system schedule    |
  | Focus    | Compute ISA                 | Data Movement ISA                            |

  Key Insight

  The data movement datapath IS the program. The compute fabric is reactive - it just executes whatever instruction tokens arrive. The intelligence is
  in:

  1. SURE Analysis → Derives optimal space-time mapping
  2. Tiling Strategy → Output/Weight/Input stationary based on tensor shapes
  3. System Schedule → Block-oriented data movement choreography
  4. Data Movement ISA → Commands to configure DMAs, BlockMovers, Streamers


  I've implemented the foundational Data Movement ISA for the KPU simulator. Here's what was created:

  New Files Created

  | File                                     | Purpose                                                                       |
  |------------------------------------------|-------------------------------------------------------------------------------|
  | include/sw/kpu/isa/data_movement_isa.hpp | ISA definition with opcodes, operands, instructions, and program structures   |
  | src/isa/data_movement_isa.cpp            | Implementation of instruction constructors and OutputStationaryProgramBuilder |
  | include/sw/kpu/isa/program_executor.hpp  | Executor that maps ISA to hardware components                                 |
  | src/isa/program_executor.cpp             | Execution engine implementation                                               |
  | tests/isa/test_data_movement_isa.cpp     | Test suite (all 5 tests passing)                                              |
  | src/isa/CMakeLists.txt                   | Build configuration                                                           |
  | tests/isa/CMakeLists.txt                 | Test build configuration                                                      |

  Key Design Decisions

  1. Memory-technology agnostic: Uses EXTERNAL instead of GDDR6 - works with DDR4, HBM, or any future memory
  2. Output-Stationary Schedule: The program builder generates the classic OS loop order:
  for ti (output rows):
    for tj (output cols):
      for tk (reduction):
        Load A[ti,tk], B[tk,tj]
        Stream to systolic array
      Drain C[ti,tj]
      Store C[ti,tj]
  3. ISA Opcodes:
    - DMA: DMA_LOAD_TILE, DMA_STORE_TILE, DMA_PREFETCH_TILE
    - BlockMover: BM_MOVE_TILE, BM_TRANSPOSE_TILE, BM_WRITEBACK_TILE
    - Streamer: STR_FEED_ROWS, STR_FEED_COLS, STR_DRAIN_OUTPUT
    - Sync: BARRIER, WAIT_*, SIGNAL
  4. Test Results (1024×1024×1024 matmul):
    - 37,889 instructions generated
    - Arithmetic intensity: 15.52 FLOPs/byte
    - Reuse factor: 11× (vs minimum 12MB, actual 132MB)

  Next Steps

  To fully simulate matmul execution, we still need:
  1. Wire the ProgramExecutor to actual hardware simulation
  2. Implement weight-stationary and input-stationary schedulers
  3. Add double-buffering optimization to overlap data movement and compute
  4. Connect to DFX compiler output for end-to-end flow


● Bash(./examples/basic/example_data_movement_isa_matmul 2>&1) timeout: 1m 0s
  ⎿  ================================================================================
                    Data Movement ISA for Domain Flow Architecture
     ================================================================================
     … +288 lines (ctrl+o to expand)

● The example runs successfully and demonstrates the Data Movement ISA. Here's a summary of what was created:

  Summary

  Created a new example examples/basic/data_movement_isa_matmul.cpp that demonstrates:

  Example 1: Small MatMul (64×64×64)

  - Shows basic program generation with 89 instructions
  - Demonstrates the output-stationary loop structure
  - Includes instruction-by-instruction trace

  Example 2: Large MatMul with Double Buffering (512×512×512)

  - 4,865 instructions generated
  - Shows traffic estimates (17 MB external, 5.67× reuse factor)
  - Arithmetic intensity: 15.06 FLOPs/byte

  Example 3: Tile Size Impact Analysis

  - Compares different tile configurations on 256×256×256
  - Shows trade-offs between instruction count and arithmetic intensity:
    - Small tiles (32³): 4865 instructions, 7.53 F/B
    - Medium tiles (64³): 641 instructions, 14.22 F/B
    - Large tiles (128³): 89 instructions, 25.60 F/B

  Example 4: Output-Stationary Loop Structure

  - Documents the OS dataflow pattern
  - Shows 4 K-iterations accumulating before C writeback
  - Explains reuse patterns for A, B, and C

  Contrast with data_movement_pipeline.cpp

  | Aspect      | data_movement_pipeline.cpp              | data_movement_isa_matmul.cpp   |
  |-------------|-----------------------------------------|--------------------------------|
  | Approach    | Direct component API calls              | ISA-driven program execution   |
  | Use case    | Testing/debugging individual components | Actual program execution model |
  | Abstraction | Low-level hardware operations           | High-level tiled schedule      |
  | Parallelism | Manual orchestration                    | Automatic from SURE analysis   |

● Bash(./examples/basic/example_data_movement_isa_matmul 2>&1 | head -100)
  ⎿  ================================================================================
                    Data Movement ISA for Domain Flow Architecture
     ================================================================================
     … +96 lines (ctrl+o to expand)

● The Data Movement ISA implementation is complete and working. The key accomplishments:

  1. Data Movement ISA (include/sw/kpu/isa/data_movement_isa.hpp, src/isa/data_movement_isa.cpp):
    - Technology-agnostic memory levels (EXTERNAL, L3, L2, L1, PE_REG)
    - Opcodes for DMA, BlockMover, Streamer operations
    - Matrix-space tile coordinates (A_tile[ti,tk], B_tile[tk,tj], C_tile[ti,tj])
    - OutputStationaryProgramBuilder for automatic schedule generation
  2. Program Executor (include/sw/kpu/isa/program_executor.hpp, src/isa/program_executor.cpp):
    - Maps ISA instructions to hardware components
    - Tracks execution state and statistics
  3. Test Suite (tests/isa/test_data_movement_isa.cpp): All 5 tests passing
  4. Example (examples/basic/data_movement_isa_matmul.cpp): Demonstrates ISA usage
  5. Specification (docs/STILLWATER_KPU_SPECIFICATION.md): Comprehensive KPU documentation


● Bash(ninja test_data_movement_isa && ./tests/isa/test_data_movement_isa 2>&1)
  ⎿  [1/1] Linking CXX executable tests/isa/test_data_movement_isa

     ************************************************************
     … +145 lines (ctrl+o to expand)

● All tests pass. Let me provide a summary of what was implemented:

  Summary

  I've implemented a concurrent execution model for the KPU simulator that properly shows all hardware resources operating in parallel. Here's what was
  added:

  New Files

  1. include/sw/kpu/isa/concurrent_executor.hpp - Header defining:
    - ResourceType enum (DMA_ENGINE, BLOCK_MOVER, STREAMER, COMPUTE_FABRIC)
    - ResourceId struct for identifying specific resources
    - ScheduledOp struct tracking operation timing on resources
    - HardwareResource class modeling bandwidth-limited resources
    - MemoryChannel struct (one DMA engine per channel)
    - ResourceConfig for configuring system resources
    - ConcurrentExecutor class for parallel execution simulation
    - TimelineFormatter for visualization output
  2. src/isa/concurrent_executor.cpp - Implementation with:
    - Parallel scheduling of operations across multiple DMA engines, BlockMovers, and Streamers
    - Barrier synchronization handling
    - Resource utilization tracking
    - Three visualization formats:
        - Gantt chart timeline showing all resources with A/B/C matrix markers
      - Occupancy table with per-resource statistics
      - Cycle-by-cycle view for detailed analysis

  Updated Files

  - src/isa/CMakeLists.txt - Added concurrent_executor.cpp
  - examples/basic/data_movement_isa_matmul.cpp - Added Example 4 demonstrating concurrent execution

  Key Insights Shown

  The new output reveals:
  - 4 DMA engines operating concurrently on different memory channels
  - 4 BlockMovers moving data L3→L2 in parallel
  - 4 Streamers feeding the systolic array concurrently
  - Resource utilization percentages (currently showing ~15% DMA, ~6.5% BM, ~3.7% STR)
  - Operations distributed across resources based on tile coordinates

  The visualization replaces the misleading sequential instruction trace with a proper concurrent view showing what each resource is doing at each cycle.


  Root Cause Analysis

  The debug output shows:

  Issue 1: DMA Operations Are Sequential, Not Parallel

  Looking at the scheduled operations:
  Start |    End |    Dur |   Resource
      0 |     20 |     20 |     DMA[0]  A
     20 |     40 |     20 |     DMA[0]  B  <-- starts after A finishes!
  The two DMA loads for A and B (which should be parallel on different channels) are running sequentially on the same DMA engine!

  Issue 2: BlockMovers Only Use BM[0]

  Operations per resource:
    BM[0]: 16 operations
  All 16 BM operations go to BM[0], leaving BM[1], BM[2], BM[3] idle.

  Issue 3: Streamers Only Use STR[0]

    STR[0]: 20 operations
  All 20 STR operations go to STR[0], leaving others idle.

  Issue 4: Barriers Create Excessive Serialization

  The barriers force complete serialization between phases, preventing overlap of:
  - DMA load of next tile with BM/STR of current tile
  - Any pipelining

  The Core Problems:

  1. Resource Selection in concurrent_executor.cpp:
    - select_dma_channel() hashes by matrix * 1000 + ti*100 + tj*10 + tk
    - But A and B operations for the same tile iteration need to go to different channels!
    - Currently A[0,0,0] and B[0,0,0] both hash to channels that collide
  2. Program Builder assigns all operations to l3_tile_id=0 and l2_bank_id=0:
    - This makes the modular selection return 0 for all BlockMovers and Streamers
  3. Operations between barriers aren't running in parallel:
    - Instructions 0 and 1 (both DMA_LOAD) should start at cycle 0 on different channels
    - But the scheduling puts them sequentially

  Would you like me to fix these issues? The fixes would be:
  1. Change resource selection to ensure A and B in the same iteration use different resources
  2. Have operations within a barrier section start at the same cycle (parallel issue)
  3. Properly assign L3 tile IDs and L2 bank IDs in the program builder

● The TileLayout integration is complete. Let me provide a summary of what was accomplished:

  Summary: TileLayout Integration Complete

  What was done:

  1. Fixed the resource selection functions in concurrent_executor.cpp:
    - select_dma_channel() now uses the tile layout's get_channel() method
    - select_block_mover() now uses the tile location's L3 ID
    - select_streamer() now uses the tile location's L2 bank ID
  2. Updated the validation logic in MatrixPartitionedLayout:
    - Removed the strict constraint that prevented C from sharing channels
    - A and B must be on separate channels (concurrent access)
    - C can share channels with A or B (accessed at different times during drain phase)
  3. Verified the integration with the debug tool showing:
    - A tiles on channels 0, 1 (DMA[0], DMA[1])
    - B tiles on channels 2, 3 (DMA[2], DMA[3])
    - BlockMovers BM[0-3] all utilized
    - Streamers STR[0-3] all utilized
    - Concurrent operations at cycle 0: DMA[0] and DMA[2] running in parallel

  Layout Policies Available:

  | Policy               | Conflict Rate | Trade-offs                                           |
  |----------------------|---------------|------------------------------------------------------|
  | MATRIX_PARTITIONED   | 0%            | Simple, predictable, may underutilize some channels  |
  | ROUND_ROBIN          | ~25%          | Even distribution, some conflicts                    |
  | ITERATION_AWARE      | 0%            | A on even, B on odd channels, optimal bandwidth      |
  | HARDWARE_INTERLEAVED | 0%            | Address-based, realistic HW model, ~5% fragmentation |

  Files Modified:

  - src/isa/concurrent_executor.cpp - Integrated TileLayout into resource selection
  - src/isa/tile_layout.cpp - Fixed validation to allow C to share channels
  - examples/basic/concurrent_execution_debug.cpp - Updated documentation

  The system is now configurable - you can switch between layout policies by creating a different TileLayout and passing it to the ConcurrentExecutor
  constructor or via set_tile_layout().

● Now I see the issue! The DMA transfer takes 4096 bytes / 12.8 GB/s = 320 cycles for a 32x32x4 tile. But we still don't see BM and STR activity in the
  first 500 cycles. This is because there's a BARRIER between DMA and BM - the BM doesn't start until the DMA completes.

  Let me check when the first BM operation starts:

● Bash(./examples/basic/example_data_movement_isa_matmul 2>&1 | grep -A600 "Cycle-by-Cycle View" | grep -E  timeout: 30s
      "^\s*(31[0-9]|32[0-9]|33[0-9]|34[0-9]|35[0-9]|36[0-9]|37[0-9…)
  ⎿       310 |  A   |  -   |  B   |  -   |  -   |  -   |  -   |  -   |  -  |  -  |  -  |  -  |
          311 |  A   |  -   |  B   |  -   |  -   |  -   |  -   |  -   |  -  |  -  |  -  |  -  |
          312 |  A   |  -   |  B   |  -   |  -   |  -   |  -   |  -   |  -  |  -  |  -  |  -  |
     … +97 lines (ctrl+o to expand)

● Now the cycle-by-cycle view is working correctly and showing the full pipeline:

  - Cycles 0-319: DMA[0] loading A, DMA[2] loading B concurrently
  - Cycles 320-383: BM[0] moving A (L3→L2), BM[2] moving B (L3→L2) concurrently
  - Cycles 384-415: STR[0] streaming A, STR[2] streaming B concurrently

  The timing is now based on realistic LPDDR5X bandwidth:
  - DMA: 4096 bytes / 12.8 GB/s = 320 cycles
  - BM: 4096 bytes / 64 GB/s = 64 cycles
  - STR: 4096 bytes / 128 GB/s = 32 cycles

● The bandwidth and timing are now correctly modeled:

  Summary of changes:

  1. Updated default bandwidth values in ResourceConfig:
    - DMA: 12.8 GB/s per channel (LPDDR5X x16 @ 8533 MT/s, conservative)
    - BlockMover: 64 GB/s (on-chip L3→L2 SRAM)
    - Streamer: 128 GB/s (on-chip L2→L1 register files)
  2. Updated Example 4 to show:
    - Total external bandwidth: 4 × 12.8 = 51.2 GB/s
    - Cycle-by-cycle view showing 500 cycles to capture full DMA→BM→STR pipeline
  3. Verified correct operation:
    - DMA[0]/DMA[2] run concurrently (A and B on separate channels)
    - BM[0]/BM[2] run concurrently after BARRIER
    - STR[0]/STR[2] run concurrently after BARRIER
    - All 4 resources of each type are now utilized

  The timing breakdown for a 32×32 tile (4096 bytes):
  - DMA: 4096 / 12.8 = 320 cycles
  - BM: 4096 / 64 = 64 cycles
  - STR: 4096 / 128 = 32 cycles


====================================================================================================
Resource Timeline (1 char = 403.2 cycles, total = 32256 cycles)
====================================================================================================

DMA Engines (External Memory ↔ L3):
DMA[0]       |A A CA A  A A CA A  A A CA A  A A CA A  A A CA A  A A CA A  A A CA A  A A CA A  |
DMA[1]       | A A  A AC A A  A AC A A  A AC A A  A AC A A  A AC A A  A AC A A  A AC A A  A AC|
DMA[2]       |BBBB      BBBB      BBBB      BBBB      BBBB      BBBB      BBBB      BBBB      |
DMA[3]       |     BBBB      BBBB      BBBB      BBBB      BBBB      BBBB      BBBB      BBBB |

Block Movers (L3 ↔ L2):
BM[0]        |A A  A A  A A  A A  A A  A A  A A  A A  A A  A A  A A  A A  A A  A A  A A  A A  |
BM[1]        | A A  A A  A A  A A  A A  A A  A A  A A  A A  A A  A A  A A  A A  A A  A A  A A |
BM[2]        |BBBB      BBBB      BBBB      BBBB      BBBB      BBBB      BBBB      BBBB      |
BM[3]        |     BBBB      BBBB      BBBB      BBBB      BBBB      BBBB      BBBB      BBBB |

Streamers (L2 ↔ L1):
STR[0]       |A  ACA  A A  ACA  A A  ACA  A A  ACA  A A  ACA  A A  ACA  A A  ACA  A A  ACA  A |
STR[1]       | A  A A  C A  A A  C A  A A  C A  A A  C A  A A  C A  A A  C A  A A  C A  A A  C|
STR[2]       |BB BB     BB BB     BB BB     BB BB     BB BB     BB BB     BB BB     BB BB     |
STR[3]       |     BB BB     BB BB     BB BB     BB BB     BB BB     BB BB     BB BB     BB BB|

Legend: A=Matrix A, B=Matrix B, C=Matrix C, ' '=Idle


> The granularity of 320 cycles of the DMA engine is clearly too coarse and would create massive bubbles downstream. So we will need to improve this.
However, we don't know how bad 320 cycles are because we do not know the cycle time of the DMA engine, the BM, the streamer, or the compute fabric. We do
 know what the cycle time of the ALUs in the compute fabric can be as low as 500psec. This would mean that a 16x16 array would need 32*4*2Ghz=256GB/sec
of ingress bandwidth. The L1 buffers would need 4GB/sec of bandwidth, which banked design and FIFOs. The L2, we know will be very wide, the full 64byte =
 512bits, and heavily banked, so that we can get away with say a 2nsec cycle time, and still generate 32GB/sec per bank, and with 8 banks we could
support the 256GB/sec ingress demand of the systolic array. So, both L1 and L2 will be operating at cycle times of 2nsec (500MHz), and thus the Streamer
and BlockMover will be operating at that cycle time. The L3 is likely half that frequency, so the DMA engines are likely running at 250MHz, or 4nsec
cycle times. Please incorporate these design configurations in the legends of the concurrency and time display so that the reader has enough context of
the bandwidths and time progression. By the way, an LPDDR/DDR/GDDR memory subsystem will be address through bursts, providing 64byte cachelines. We can
assume that the DMA engine is moving 64byte cycles in a single cycle so it will have 512 bit busses to the L3s, possibly done through a ring bus so that
we can move big packets around the checkerboard without contention.

● Excellent analysis! Let me incorporate these design parameters into the system. The key insights are:

  1. Compute fabric ALUs: 2 GHz (500 ps cycle time)
  2. L1/L2/Streamer/BlockMover: 500 MHz (2 ns cycle time)
  3. L3/DMA engines: 250 MHz (4 ns cycle time)
  4. DMA burst size: 64 bytes (cache line) per cycle at 250 MHz = 16 GB/s per channel
  5. Systolic array ingress demand: 256 GB/s (32 elements × 4 bytes × 2 GHz)


● The timing is now correct and well-documented. The display shows:

  - DMA completes at cycle 256 (4096 bytes ÷ 16 GB/s = 256 DMA cycles × 4 ns = 1024 ns)
  - BM starts at cycle 256 and runs concurrently (A on BM[0], B on BM[2])
  - BM takes 128 cycles (4096 bytes ÷ 32 GB/s = 128 cycles, but at 500 MHz = 64 DMA cycles)

  Wait, there's a discrepancy - I should verify the cycle counting. The BM runs at 500 MHz (2 ns/cycle) while DMA runs at 250 MHz (4 ns/cycle). So 1 DMA
  cycle = 2 BM cycles. The display is in DMA cycles, so BM operations should appear half as long. Let me check the STR transition:

● Bash(./examples/basic/example_data_movement_isa_matmul 2>&1 | grep -A600 "Cycle-by-Cycle View" | grep -E                                       timeout:
      "^\s*(37[0-9]|38[0-9]|39[0-9]|40[0-9]) \|")                                                                                                30s
  ⎿       370 |  -   |  -   |  -   |  -   |  A   |  -   |  B   |  -   |  -  |  -  |  -  |  -  |
          371 |  -   |  -   |  -   |  -   |  A   |  -   |  B   |  -   |  -  |  -  |  -  |  -  |
          372 |  -   |  -   |  -   |  -   |  A   |  -   |  B   |  -   |  -  |  -  |  -  |  -  |
     … +37 lines (ctrl+o to expand)

● The timing shows:
  - DMA: cycles 0-255 (256 cycles × 4 ns = 1024 ns)
  - BM: cycles 256-383 (128 cycles × 4 ns = 512 ns)
  - STR: cycles 384-... (128 cycles × 4 ns = 512 ns)

  This is exactly right! The cycle counts in the display are expressed in DMA cycles (the slowest clock), so:
  - BM takes 128 DMA cycles = 256 BM cycles (at 500 MHz) = 4096 bytes / 32 GB/s ✓
  - STR takes 128 DMA cycles = 256 STR cycles (at 500 MHz) = 4096 bytes / 32 GB/s ✓

  The display now provides comprehensive context including:
  1. Clock domains with frequencies and cycle times
  2. Bus widths (64-byte cache lines)
  3. Per-resource and aggregate bandwidths
  4. Total execution time in both cycles and nanoseconds/microseconds
  5. Scale information showing how cycle times map to real time

● The implementation now includes realistic design parameters:

  Summary of Updates:

  1. Clock Domain Hierarchy:
    - Compute fabric: 2.0 GHz (500 ps cycle)
    - L1/L2/Streamer/BlockMover: 500 MHz (2 ns cycle)
    - L3/DMA engines: 250 MHz (4 ns cycle)
  2. Bus Architecture:
    - All data movers use 64-byte (512-bit) buses for cache-line transfers
    - DMA: 64 bytes × 250 MHz = 16 GB/s per channel
    - BM: 64 bytes × 500 MHz = 32 GB/s per mover
    - STR: 64 bytes × 500 MHz = 32 GB/s per streamer
  3. Aggregate Bandwidth:
    - External: 4 channels × 16 GB/s = 64 GB/s
    - L3→L2: 4 movers × 32 GB/s = 128 GB/s
    - L2→L1: 4 streamers × 32 GB/s = 128 GB/s
  4. Display Enhancements:
    - Clock domain legend with frequencies and cycle times
    - Timeline shows total time in ns/µs
    - Cycle-by-cycle view header shows time range in nanoseconds
    - Occupancy table shows aggregate bandwidth



  Summary: L2 Cache Model for GPU

  New Components Added

  1. OperatorType enum (base.py:99-125)
  Classifies DNN operators by their cache behavior:
  - HIGH_REUSE: MatMul, Conv2D - tiled with O(n) reuse
  - LOW_REUSE: MatVec, pooling - limited reuse O(1) to O(sqrt(n))
  - STREAMING: Activation, BatchNorm, Softmax - O(1) reuse, flushes L2

  2. compute_l2_hit_ratio() function (base.py:179-279)
  Computes L2 hit ratio based on:
  - Operator type (determines reuse pattern)
  - Working set size relative to L2 cache size
  - Whether L2 is "cold" (flushed by previous streaming op)

  3. will_flush_l2() function (base.py:282-307)
  Determines if an operator will flush L2 contents, which affects the next operator.

  4. Updated build_gpu_cycle_energy() (gpu.py)
  New parameters:
  - operator_type: Type of operator
  - working_set_bytes: Size of working set
  - l2_cache_bytes: L2 cache size (default 40MB)
  - l2_is_cold: Whether L2 was flushed by previous op

  Key Insights Captured

  | Operator      | Working Set | L2 Hit | Notes                       |
  |---------------|-------------|--------|-----------------------------|
  | MatMul (warm) | 10MB        | 95%    | Tiles fit, high reuse       |
  | MatMul (cold) | 10MB        | 80%    | After ReLU flushed L2       |
  | MatMul        | 100MB       | 70%    | Exceeds L2, capacity misses |
  | ReLU          | 20MB        | 5%     | Streaming, no reuse         |
  | ReLU          | 100MB       | 5%     | Streaming, flushes L2       |
  | MatVec        | 20MB        | 60%    | Limited reuse               |
  | MatVec        | 100MB       | 10%    | Exceeds L2                  |

  Validation Result

  The model correctly shows:
  - MatMul after ReLU uses 6.5% more energy due to cold L2
  - Streaming operators (ReLU) have very low L2 hit ratio (~5%)
  - MatMul with proper tiling maintains high L2 hit ratio (~95%)

  This addresses your concern that streaming operators wipe the L2 contents, forcing the next MatMul to reload from HBM.


● The cache behavior model should be generalized across all architectures. Let me think through this:

  Memory Hierarchy by Architecture:

  | Architecture | L1/Local        | L2                      | L3               | Off-chip |
  |--------------|-----------------|-------------------------|------------------|----------|
  | CPU          | L1 (per-core)   | L2 (per-core or shared) | L3/LLC (shared)  | DRAM     |
  | GPU          | Shared Mem/L1   | L2 (shared)             | -                | HBM      |
  | TPU          | VMEM (per-core) | -                       | SRAM (shared)    | HBM      |
  | KPU          | Tile scratchpad | Global scratchpad       | Streaming buffer | DRAM     |

  Key insight: The operator type determines cache behavior regardless of architecture:
  - HIGH_REUSE ops benefit from whatever on-chip memory exists
  - STREAMING ops flush whatever implicit caches exist
  - The effect propagates to subsequent operators


● Now we can see the key insight:

  For HIGH_REUSE (MatMul): TPU wins (5.70 pJ/op) because systolic arrays are optimized for this.

  For STREAMING (ReLU): KPU wins (7.37 pJ/op) because:
  1. Explicit memory doesn't get "flushed" by streaming operators
  2. No cache coherence overhead
  3. Same energy regardless of operator type

  The TPU's energy jumped from 5.70 to 32.85 pJ/op for streaming because the SRAM hit ratio dropped from 95% to 5%.

● Let me provide a summary of all the changes made:

  Summary

  Changes Made

  1. Generalized Cache/Memory Model (base.py)

  Added new types and functions:
  - OperatorType enum: HIGH_REUSE, LOW_REUSE, STREAMING
  - MemoryType enum: L2, L3
  - Cache size constants for all architectures:
    - GPU_L2_CACHE_SIZES
    - CPU_L3_CACHE_SIZES
    - TPU_L2_SRAM_SIZES
    - KPU_L1_STREAMING_BUFFER_SIZES, KPU_L2_TILE_STAGING_SIZES, KPU_L3_GLOBAL_SCRATCHPAD_SIZES
  - compute_cache_hit_ratio(): Generalized function for computing hit ratios
  - will_flush_cache(): Determines if operator will flush cache

  2. Updated Architecture Models

  Each model now accepts:
  - operator_type: Type of DNN operator
  - working_set_bytes: Size of working set
  - cache_is_cold / l2_is_cold / l3_is_cold / sram_is_cold: Whether previous op flushed cache

  | Architecture | Cache Level          | Behavior                                      |
  |--------------|----------------------|-----------------------------------------------|
  | CPU          | L3 (LLC)             | Implicit, operator-dependent hit ratio        |
  | GPU          | L2                   | Implicit, operator-dependent hit ratio        |
  | TPU          | L2 (SRAM)            | Implicit, operator-dependent hit ratio        |
  | KPU          | L1/L2/L3 scratchpads | Explicit (compiler-managed), binary fit/spill |

  3. Key Insight: Implicit vs Explicit Memory

  | Memory Type | Architectures | Behavior                                                                                     |
  |-------------|---------------|----------------------------------------------------------------------------------------------|
  | Implicit    | CPU, GPU, TPU | Hardware-managed caches. Streaming ops flush cache, causing "cold" penalty for next op.      |
  | Explicit    | KPU           | Compiler-managed scratchpads. No flush problem. Same energy regardless of operator sequence. |

  4. Validation Results

  For 100K ops, 4MB working set:

  | Operator   | CPU     | GPU     | TPU     | KPU    |
  |------------|---------|---------|---------|--------|
  | HIGH_REUSE | 39.6 pJ | 14.9 pJ | 5.7 pJ  | 7.4 pJ |
  | STREAMING  | 40.5 pJ | 19.7 pJ | 32.9 pJ | 7.4 pJ |

  - TPU wins for MatMul (systolic efficiency)
  - KPU wins for streaming ops (no cache flush penalty)
  - KPU energy is constant regardless of operator type



